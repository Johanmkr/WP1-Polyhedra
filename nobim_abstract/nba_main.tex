
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{import}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multicol}

\title{Can a Convex Partition caused by a CPWL Neural Network be used for Density Estimation?}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Johan Mylius-Kroken}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

% \begin{abstract}
% This is the abstract.
% \end{abstract}
The increasing popularity of deep learning methods has led to a growing interest in understanding the theoretical properties of neural network. The use of information theory, as proposed by~\cite{tishbyDeepLearningInformation2015a,shwartz-zivOpeningBlackBox2017}, provides a promising framework for studying the behaviour of these models. However, calculation of information theoretic (IT) quantities rely on estimating high-dimensional probability densities, which is challenging. ~\cite{geigerInformationPlaneAnalyses2022} show that the numerical results of different IT-measures vary depending on which estimator is used. In an attempt to combat this, we want to use the convex partitioning properties of Continuous Piecewise Linear (CPWL) neural network, such as ReLU networks, in order to create ``natural bins'' of the models input and latent spaces. 

A ReLU network will partition its input space into convex regions~\citep{serraBoundingCountingLinear2018,haninComplexityLinearRegions2019}. The number of regions is upper bounded by $2^H$ where $H$ is the total number of hidden units in the network. However, only a few of these are considered feasible regions~\cite{haninDeepReLUNetworks2019}. Finding these regions for the whole network is previously explored by ~\cite{liuReLUNeuralNetworks2023,sattelbergLocallyLinearAttributes2023,humayunSplineCamExactVisualization2024}. However, all of these approaches are limited to specific cases, or weak numerical implementations. Our goal is to create a more general and scalable algorithm for finding the feasible regions of ReLU network, both for the whole network and for each layer. The ultimate goal is to use these regions as bins when performing density estimation for IT-measures. See Figure~\ref{fig:regions} for a preliminary example of how a random layer with three neurons partitions its two-dimensional input space.

If successful, this could provide a more principled way of estimating IT-measures for neural networks, free from arbitrary choices of binning or kernel sizes and other hyperparameters. In addition, we would be able to compute the mutual information between input and output, based on how the network partitions the input space. We hypothesize that mutual information should increase as the network learns how to partition input space in a feasible way. Furthermore, we should be able to spot bottlenecks in the network, by looking at how the number of regions changes from layer to layer throughout training.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/regions.png}
    \caption{Preliminary example. Random layer, $\mathbb{R}^2\rightarrow\mathbb{R}^3$, divides its input space into 7 convex regions. The activation vector $q=[1,0,1]$ (indicating which neuron is active) corresponds to the red region. Each activation pattern corresponds to a convex region in input space, of which there can be at most $2^{H}$. Only a subset are feasible. For more complex layers and networks, the idea is that these regions can be used as ``natural bins'' when estimating information theoretic quantities.}
    \label{fig:regions}
\end{figure}

% \newpage
{\tiny\bibliography{Polyhedral_Decomposition}}
\bibliographystyle{iclr2025_conference}

\end{document}
