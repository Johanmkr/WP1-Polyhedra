
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{import}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{subcaption}

\title{Can a Convex Partition caused by a CPWL Neural Network be used for Density Estimation?}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Johan Mylius-Kroken}  

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

% \begin{abstract}
% This is the abstract.
% \end{abstract}
The increasing popularity of deep learning methods has led to a growing interest in understanding the theoretical properties of neural network. The use of information theory, as proposed by~\cite{tishbyDeepLearningInformation2015a,shwartz-zivOpeningBlackBox2017}, provides a promising framework for studying the behaviour of these models. However, calculation of information theoretic (IT) quantities rely on estimating high-dimensional probability densities, which is challenging. ~\cite{geigerInformationPlaneAnalyses2022} show that the numerical result of different IT-measures varies depending on which estimator is used. In attempt to combat this, we want to use the convex partitioning properties of Continous Piecewise Linear (CPWL) neural network, such as ReLU networks, in order to create ``natural bins'' of the models input and latent spaces. 

A ReLU network will partition its input space into convex regions~\citep{serraBoundingCountingLinear2018,haninComplexityLinearRegions2019}. The number of regions is upper bounded by $2^H$ where $H$ is the total number of hidden units in the network. However, only a small number of these are considered feasible regions~\cite{haninDeepReLUNetworks2019}. Finding these regions for the whole network is previously explored by ~\cite{liuReLUNeuralNetworks2023,sattelbergLocallyLinearAttributes2023,humayunSplineCamExactVisualization2024}. However, all of these approaches are limited to specific cases, or weak numerical implementations. Our goal is to create a more general and scalable algorithm for finding the feasible regions of ReLU network, both for the whole network and for each layer. The ultimate goal is to use these regions as bins when performing density estimation for IT-measures. 

If successfull, this could provide a more principled way of estimating IT-measures for neural networks, free from arbitrary choices of binning or kernel sizes and other hyperparamaters. In addition, we would be able to compute the mutual information between the input and output, based on how the network partitions input space. The hypothesis is that this should increase as the network learns how to partition input space in a feasible way. Furthermore, we should be able to spot bottlenecks in the network, by looking at how the number of regions changes from layer to layer, and which layers that causes the most reduction in number of regions.

% \section{Information Theory}
% \import{sections/}{informationtheory.tex}

% \section{Experiment}
% \import{sections/}{experiment.tex}

% \newpage
{\tiny\bibliography{Polyhedral_Decomposition}}
\bibliographystyle{iclr2025_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.
 

\end{document}
