@misc{arvanitidisGeometricallyEnrichedLatent2020,
  title = {Geometrically {{Enriched Latent Spaces}}},
  author = {Arvanitidis, Georgios and Hauberg, S{\o}ren and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = aug,
  number = {arXiv:2008.00565},
  eprint = {2008.00565},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.00565},
  urldate = {2025-02-14},
  abstract = {A common assumption in generative models is that the generator immerses the latent space into a Euclidean ambient space. Instead, we consider the ambient space to be a Riemannian manifold, which allows for encoding domain knowledge through the associated Riemannian metric. Shortest paths can then be defined accordingly in the latent space to both follow the learned manifold and respect the ambient geometry. Through careful design of the ambient metric we can ensure that shortest paths are well-behaved even for deterministic generators that otherwise would exhibit a misleading bias. Experimentally we show that our approach improves interpretability of learned representations both using stochastic and deterministic generators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/46JFZEFY/Arvanitidis et al. - 2020 - Geometrically Enriched Latent Spaces.pdf;/home/johan/Zotero/storage/7XBFL5BG/2008.html}
}

@misc{arvanitidisPullingBackInformation2022,
  title = {Pulling Back Information Geometry},
  author = {Arvanitidis, Georgios and {Gonz{\'a}lez-Duque}, Miguel and Pouplin, Alison and Kalatzis, Dimitris and Hauberg, S{\o}ren},
  year = {2022},
  month = apr,
  number = {arXiv:2106.05367},
  eprint = {2106.05367},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.05367},
  urldate = {2025-01-21},
  abstract = {Latent space geometry has shown itself to provide a rich and rigorous framework for interacting with the latent variables of deep generative models. The existing theory, however, relies on the decoder being a Gaussian distribution as its simple reparametrization allows us to interpret the generating process as a random projection of a deterministic manifold. Consequently, this approach breaks down when applied to decoders that are not as easily reparametrized. We here propose to use the Fisher-Rao metric associated with the space of decoder distributions as a reference metric, which we pull back to the latent space. We show that we can achieve meaningful latent geometries for a wide range of decoder distributions for which the previous theory was not applicable, opening the door to `black box' latent geometries.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/4N7IZIN2/Arvanitidis et al. - 2022 - Pulling back information geometry.pdf}
}

@article{bronsteinGeometricDeepLearning2017,
  title = {Geometric Deep Learning: Going beyond {{Euclidean}} Data},
  shorttitle = {Geometric Deep Learning},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  year = {2017},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {4},
  eprint = {1611.08097},
  primaryclass = {cs},
  pages = {18--42},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2017.2693418},
  urldate = {2025-02-17},
  abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/johan/Zotero/storage/UFCSWSXF/Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean data.pdf}
}

@misc{chazalIntroductionTopologicalData2021,
  title = {An Introduction to {{Topological Data Analysis}}: Fundamental and Practical Aspects for Data Scientists},
  shorttitle = {An Introduction to {{Topological Data Analysis}}},
  author = {Chazal, Fr{\'e}d{\'e}ric and Michel, Bertrand},
  year = {2021},
  month = feb,
  number = {arXiv:1710.04019},
  eprint = {1710.04019},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.04019},
  urldate = {2025-01-22},
  abstract = {Topological Data Analysis (tda) is a recent and fast growing field providing a set of new topological and geometric tools to infer relevant features for possibly complex data. This paper is a brief introduction, through a few selected topics, to basic fundamental and practical aspects of tda for non experts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/home/johan/Zotero/storage/8DB5T8KP/Chazal and Michel - 2021 - An introduction to Topological Data Analysis fund.pdf}
}

@misc{combeExploringInformationGeometry2025,
  title = {Exploring Information Geometry: {{Recent Advances}} and {{Connections}} to {{Topological Field Theory}}},
  shorttitle = {Exploring Information Geometry},
  author = {Combe, No{\'e}mie C. and Combe, Philippe G. and Nencka, Hanna K.},
  year = {2025},
  month = feb,
  number = {arXiv:2502.11188},
  eprint = {2502.11188},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.11188},
  urldate = {2025-02-24},
  abstract = {This introductory text arises from a lecture given in G{\textbackslash}"oteborg, Sweden, given by the first author and is intended for undergraduate students, as well as for any mathematically inclined reader wishing to explore a synthesis of ideas connecting geometry and statistics. At its core, this work seeks to illustrate the profound and yet natural interplay between differential geometry, probability theory, and the rich algebraic structures encoded in (pre-)Frobenius manifolds. The exposition is structured into three principal parts. The first part provides a concise introduction to differential topology and geometry, emphasizing the role of smooth manifolds, connections, and curvature in the formulation of geometric structures. The second part is devoted to probability, measures, and statistics, where the notion of a probability space is refined into a geometric object, thus paving the way for a deeper mathematical understanding of statistical models. Finally, in the third part, we introduce (pre-)Frobenius manifolds, revealing their surprising connection to exponential families of probability distributions and, discuss more broadly, their role in the geometry of information. At the end of those three parts the reader will find stimulating exercises. By bringing together these seemingly distant disciplines, we aim to highlight the natural emergence of geometric structures in statistical theory. This work does not seek to be exhaustive but rather to provide the reader with a pathway into a domain of mathematics that is still in its formative stages, where many fundamental questions remain open. The text is accessible without requiring advanced prerequisites and should serve as an invitation to further exploration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Mathematics - Algebraic Geometry,Mathematics - Differential Geometry,Mathematics - Information Theory},
  file = {/home/johan/Zotero/storage/R5QA2VPP/Combe et al. - 2025 - Exploring information geometry Recent Advances and Connections to Topological Field Theory.pdf;/home/johan/Zotero/storage/G64JBW6W/2502.html}
}

@misc{fuLatentTopologyInduction2022,
  title = {Latent {{Topology Induction}} for {{Understanding Contextualized Representations}}},
  author = {Fu, Yao and Lapata, Mirella},
  year = {2022},
  month = jun,
  number = {arXiv:2206.01512},
  eprint = {2206.01512},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.01512},
  urldate = {2025-01-22},
  abstract = {In this work, we study the representation space of contextualized embeddings and gain insight into the hidden topology of large language models. We show there exists a network of latent states that summarize linguistic properties of contextualized representations. Instead of seeking alignments to existing welldefined annotations, we infer this latent network in a fully unsupervised way using a structured variational autoencoder. The induced states not only serve as anchors that mark the topology (neighbors and connectivity) of the representation manifold but also reveals the internal mechanism of encoding sentences. With the induced network, we: (1). decompose the representation space into a spectrum of latent states which encode fine-grained word meanings with lexical, morphological, syntactic and semantic information; (2). show state-state transitions encode rich phrase constructions and serve as the backbones of the latent space. Putting the two together, we show that sentences are represented as a traversal over the latent network where state-state transition chains encode syntactic templates and stateword emissions fill in the content. We demonstrate these insights with extensive experiments and visualizations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/johan/Zotero/storage/8WZQNTGH/Fu and Lapata - 2022 - Latent Topology Induction for Understanding Contex.pdf}
}

@misc{kianiHardnessLearningNeural2024,
  title = {Hardness of {{Learning Neural Networks}} under the {{Manifold Hypothesis}}},
  author = {Kiani, Bobak T. and Wang, Jason and Weber, Melanie},
  year = {2024},
  month = jun,
  number = {arXiv:2406.01461},
  eprint = {2406.01461},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.01461},
  urldate = {2025-01-22},
  abstract = {The manifold hypothesis presumes that high-dimensional data lies on or near a low-dimensional manifold. While the utility of encoding geometric structure has been demonstrated empirically, rigorous analysis of its impact on the learnability of neural networks is largely missing. Several recent results have established hardness results for learning feedforward and equivariant neural networks under i.i.d. Gaussian or uniform Boolean data distributions. In this paper, we investigate the hardness of learning under the manifold hypothesis. We ask which minimal assumptions on the curvature and regularity of the manifold, if any, render the learning problem efficiently learnable. We prove that learning is hard under input manifolds of bounded curvature by extending proofs of hardness in the SQ and cryptographic settings for Boolean data inputs to the geometric setting. On the other hand, we show that additional assumptions on the volume of the data manifold alleviate these fundamental limitations and guarantee learnability via a simple interpolation argument. Notable instances of this regime are manifolds which can be reliably reconstructed via manifold learning. Looking forward, we comment on and empirically explore intermediate regimes of manifolds, which have heterogeneous features commonly found in real world data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Differential Geometry,Statistics - Machine Learning}
}

@misc{meilaManifoldLearningWhat2023,
  title = {Manifold Learning: What, How, and Why},
  shorttitle = {Manifold Learning},
  author = {Meil{\u a}, Marina and Zhang, Hanyu},
  year = {2023},
  month = nov,
  number = {arXiv:2311.03757},
  eprint = {2311.03757},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.03757},
  urldate = {2025-01-22},
  abstract = {Manifold learning (ML), known also as non-linear dimension reduction, is a set of methods to find the low dimensional structure of data. Dimension reduction for large, high dimensional data is not merely a way to reduce the data; the new representations and descriptors obtained by ML reveal the geometric shape of high dimensional point clouds, and allow one to visualize, denoise and interpret them. This survey presents the principles underlying ML, the representative methods, as well as their statistical foundations from a practicing statistician's perspective. It describes the trade-offs, and what theory tells us about the parameter and algorithmic choices we make in order to obtain reliable conclusions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/TDWD5BG9/MeilÄƒ and Zhang - 2023 - Manifold learning what, how, and why.pdf}
}

@article{nielsenElementaryIntroductionInformation2020,
  title = {An {{Elementary Introduction}} to {{Information Geometry}}},
  author = {Nielsen, Frank},
  year = {2020},
  month = oct,
  journal = {Entropy},
  volume = {22},
  number = {10},
  pages = {1100},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e22101100},
  urldate = {2025-01-21},
  abstract = {In this survey, we describe the fundamental differential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some use cases of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of differential geometry. Proofs are omitted for brevity.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {-embeddings,affine connection,Bayesian hypothesis testing,conjugate connections,curvature and flatness,differential geometry,dual metric-compatible parallel transport,dually flat manifolds,exponential family,Fisher-Rao distance,gauge freedom,Hessian manifolds,information manifold,metric compatibility,metric tensor,mixed parameterization,mixture clustering,mixture family,parameter divergence,separable divergence,statistical divergence,statistical invariance,statistical manifold},
  file = {/home/johan/Zotero/storage/QW5UFEW4/Nielsen - 2020 - An Elementary Introduction to Information Geometry.pdf}
}

@misc{pouplinIdentifyingLatentDistances2023,
  title = {Identifying Latent Distances with {{Finslerian}} Geometry},
  author = {Pouplin, Alison and Eklund, David and Ek, Carl Henrik and Hauberg, S{\o}ren},
  year = {2023},
  month = oct,
  number = {arXiv:2212.10010},
  eprint = {2212.10010},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10010},
  urldate = {2025-01-21},
  abstract = {Riemannian geometry provides us with powerful tools to explore the latent space of generative models while preserving the underlying structure of the data. The latent space can be equipped it with a Riemannian metric, pulled back from the data manifold. With this metric, we can systematically navigate the space relying on geodesics defined as the shortest curves between two points.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/johan/Zotero/storage/AACTV4ZU/Pouplin et al. - 2023 - Identifying latent distances with Finslerian geome.pdf}
}

@misc{villarFullyCovariantMachine2023,
  title = {Towards Fully Covariant Machine Learning},
  author = {Villar, Soledad and Hogg, David W. and Yao, Weichi and Kevrekidis, George A. and Sch{\"o}lkopf, Bernhard},
  year = {2023},
  month = jun,
  number = {arXiv:2301.13724},
  eprint = {2301.13724},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.13724},
  urldate = {2025-02-24},
  abstract = {Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. Our goal is to understand the implications for machine learning of the many passive symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling, and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. This paper is conceptual: It translates among the languages of physics, mathematics, and machine-learning. We believe that consideration and implementation of passive symmetries might help machine learning in the same ways that it transformed physics in the twentieth century.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,Mathematical Physics,Mathematics - Mathematical Physics,Physics - Data Analysis Statistics and Probability,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/M8E6GLTA/Villar et al. - 2023 - Towards fully covariant machine learning.pdf;/home/johan/Zotero/storage/GB23X3CF/2301.html}
}
