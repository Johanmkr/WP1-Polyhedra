@inproceedings{balestrieroSplineTheoryDeep2018,
  title = {A {{Spline Theory}} of {{Deep Learning}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Balestriero, Randall and {baraniuk}},
  year = {2018},
  month = jul,
  pages = {374--383},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-09-02},
  abstract = {We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.},
  langid = {english},
  file = {/home/johan/Zotero/storage/4YLLBF4I/Balestriero and baraniuk - 2018 - A Spline Theory of Deep Learning.pdf;/home/johan/Zotero/storage/SQB4AT8Y/Balestriero and baraniuk - 2018 - A Spline Theory of Deep Learning.pdf}
}

@article{geigerInformationPlaneAnalyses2022,
  title = {On {{Information Plane Analyses}} of {{Neural Network Classifiers}} -- {{A Review}}},
  author = {Geiger, Bernhard C.},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {12},
  eprint = {2003.09671},
  primaryclass = {cs},
  pages = {7039--7051},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2021.3089037},
  urldate = {2025-01-17},
  abstract = {We review the current literature concerned with information plane analyses of neural network classifiers. While the underlying information bottleneck theory and the claim that information-theoretic compression is causally linked to generalization are plausible, empirical evidence was found to be both supporting and conflicting. We review this evidence together with a detailed analysis of how the respective information quantities were estimated. Our survey suggests that compression visualized in information planes is not necessarily information-theoretic, but is rather often compatible with geometric compression of the latent representations. This insight gives the information plane a renewed justification.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Information Theory,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/L3DX5KFV/Geiger - 2022 - On Information Plane Analyses of Neural Network Cl.pdf}
}

@misc{goujonNumberRegionsPiecewise2023,
  title = {On the {{Number}} of {{Regions}} of {{Piecewise Linear Neural Networks}}},
  author = {Goujon, Alexis and Etemadi, Arian and Unser, Michael},
  year = {2023},
  month = dec,
  number = {arXiv:2206.08615},
  eprint = {2206.08615},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.08615},
  urldate = {2025-01-20},
  abstract = {Many feedforward neural networks (NNs) generate continuous and piecewise-linear (CPWL) mappings. Specifically, they partition the input domain into regions on which the mapping is affine. The number of these so-called linear regions offers a natural metric to characterize the expressiveness of CPWL NNs. The precise determination of this quantity is often out of reach in practice, and bounds have been proposed for specific architectures, including for ReLU and Maxout NNs. In this work, we generalize these bounds to NNs with arbitrary and possibly multivariate CPWL activation functions. We first provide upper and lower bounds on the maximal number of linear regions of a CPWL NN given its depth, width, and the number of linear regions of its activation functions. Our results rely on the combinatorial structure of convex partitions and confirm the distinctive role of depth which, on its own, is able to exponentially increase the number of regions. We then introduce a complementary stochastic framework to estimate the average number of linear regions produced by a CPWL NN. Under reasonable assumptions, the expected density of linear regions along any 1D path is bounded by the product of depth, width, and a measure of activation complexity (up to a scaling factor). This yields an identical role to the three sources of expressiveness: no exponential growth with depth is observed anymore.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Combinatorics,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/home/johan/Zotero/storage/6G3CA2US/Goujon et al. - 2023 - On the Number of Regions of Piecewise Linear Neura.pdf}
}

@misc{haninComplexityLinearRegions2019,
  title = {Complexity of {{Linear Regions}} in {{Deep Networks}}},
  author = {Hanin, Boris and Rolnick, David},
  year = {2019},
  month = jun,
  number = {arXiv:1901.09021},
  eprint = {1901.09021},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.09021},
  urldate = {2025-02-24},
  abstract = {It is well-known that the expressivity of a neural network depends on its architecture, with deeper networks expressing more complex functions. In the case of networks that compute piecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of expressivity. It is possible to construct networks with merely a single region, or for which the number of linear regions grows exponentially with depth; it is not clear where within this range most networks fall in practice, either before or after training. In this paper, we provide a mathematical framework to count the number of linear regions of a piecewise linear network and measure the volume of the boundaries between these regions. In particular, we prove that for networks at initialization, the average number of regions along any one-dimensional subspace grows linearly in the total number of neurons, far below the exponential upper bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exponential, an intuition that matches our empirical observations. We conclude that the practical expressivity of neural networks is likely far below that of the theoretical maximum, and that this gap can be quantified.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/N25GTTW3/Hanin and Rolnick - 2019 - Complexity of Linear Regions in Deep Networks.pdf;/home/johan/Zotero/storage/U6C5PQGT/1901.html}
}

@misc{haninDeepReLUNetworks2019,
  title = {Deep {{ReLU Networks Have Surprisingly Few Activation Patterns}}},
  author = {Hanin, Boris and Rolnick, David},
  year = {2019},
  month = oct,
  number = {arXiv:1906.00904},
  eprint = {1906.00904},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.00904},
  urldate = {2025-02-24},
  abstract = {The success of deep networks has been attributed in part to their expressivity: per parameter, deep networks can approximate a richer class of functions than shallow networks. In ReLU networks, the number of activation patterns is one measure of expressivity; and the maximum number of patterns grows exponentially with the depth. However, recent work has showed that the practical expressivity of deep networks - the functions they can learn rather than express - is often far from the theoretical maximum. In this paper, we show that the average number of activation patterns for ReLU networks at initialization is bounded by the total number of neurons raised to the input dimension. We show empirically that this bound, which is independent of the depth, is tight both at initialization and during training, even on memorization tasks that should maximize the number of activation patterns. Our work suggests that realizing the full expressivity of deep networks may not be possible in practice, at least with current methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {/home/johan/Zotero/storage/Z5ZX972Q/Hanin and Rolnick - 2019 - Deep ReLU Networks Have Surprisingly Few Activation Patterns.pdf;/home/johan/Zotero/storage/YV7UYCC4/1906.html}
}

@misc{humayunSplineCamExactVisualization2024,
  title = {{{SplineCam}}: {{Exact Visualization}} and {{Characterization}} of {{Deep Network Geometry}} and {{Decision Boundaries}}},
  shorttitle = {{{SplineCam}}},
  author = {Humayun, Ahmed Imtiaz and Balestriero, Randall and Balakrishnan, Guha and Baraniuk, Richard},
  year = {2024},
  month = jun,
  number = {arXiv:2302.12828},
  eprint = {2302.12828},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.12828},
  urldate = {2025-05-05},
  abstract = {Current Deep Network (DN) visualization and interpretability methods rely heavily on data space visualizations such as scoring which dimensions of the data are responsible for their associated prediction or generating new data features or samples that best match a given DN unit or representation. In this paper, we go one step further by developing the first provably exact method for computing the geometry of a DN's mapping -- including its decision boundary -- over a specified region of the data space. By leveraging the theory of Continuous Piece-Wise Linear (CPWL) spline DNs, SplineCam exactly computes a DN's geometry without resorting to approximations such as sampling or architecture simplification. SplineCam applies to any DN architecture based on CPWL activation nonlinearities, including (leaky) ReLU, absolute value, maxout, and maxpooling and can also be applied to regression DNs such as implicit neural representations. Beyond decision boundary visualization and characterization, SplineCam enables one to compare architectures, measure generalizability, and sample from the decision boundary on or off the data manifold. Project website: bit.ly/splinecam.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/johan/Zotero/storage/BBACG9J5/Humayun et al. - 2024 - SplineCam Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries.pdf}
}

@article{liuReLUNeuralNetworks2023,
  title = {{{ReLU Neural Networks}}, {{Polyhedral Decompositions}}, and {{Persistent Homology}}},
  author = {Liu, Yajing and Cole, Christina M. and Peterson, Chris and Kirby, Michael},
  year = {2023},
  month = jun,
  urldate = {2025-06-02},
  abstract = {A ReLU neural network leads to a finite polyhedral decomposition of input space and a corresponding finite dual graph. We show that while this dual graph is a coarse quantization of input space, it is sufficiently robust that it can be combined with persistent homology to detect homological signals of manifolds in the input space from samples. This property holds for a wide range of networks trained for a wide range of purposes that have nothing to do with this topological application. We found this feature to be surprising and interesting; we hope it will also be useful.},
  langid = {english},
  file = {/home/johan/Zotero/storage/5EKB47X7/Liu et al. - 2023 - ReLU Neural Networks, Polyhedral Decompositions, and Persistent Homology.pdf}
}

@misc{pascanuNumberResponseRegions2014,
  title = {On the Number of Response Regions of Deep Feed Forward Networks with Piece-Wise Linear Activations},
  author = {Pascanu, Razvan and Montufar, Guido and Bengio, Yoshua},
  year = {2014},
  month = feb,
  number = {arXiv:1312.6098},
  eprint = {1312.6098},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6098},
  urldate = {2025-01-20},
  abstract = {This paper explores the complexity of deep feedforward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectifier multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has kn hidden units and n0 inputs, then the number of linear regions is O(kn0 nn0 ). For a k layer model with n hidden units on each layer it is {\textohm}( n/n0 k-1 nn0 ). The number n/n0 k-1 grows faster than kn0 when n tends to infinity or when k tends to infinity and n {$\geq$} 2n0. Additionally, even when k is small, if we restrict n to be 2n0, we can show that a deep model has considerably more linear regions that a shallow one. We consider this as a first step towards understanding the complexity of these models and specifically towards providing suitable mathematical tools for future analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/johan/Zotero/storage/BI8FU8DA/Pascanu et al. - 2014 - On the number of response regions of deep feed for.pdf}
}

@article{sattelbergLocallyLinearAttributes2023,
  title = {Locally Linear Attributes of {{ReLU}} Neural Networks},
  author = {Sattelberg, Ben and Cavalieri, Renzo and Kirby, Michael and Peterson, Chris and Beveridge, Ross},
  year = {2023},
  month = nov,
  journal = {Frontiers in Artificial Intelligence},
  volume = {6},
  publisher = {Frontiers},
  issn = {2624-8212},
  doi = {10.3389/frai.2023.1255192},
  urldate = {2025-06-23},
  abstract = {A ReLU neural network functions as a continuous piecewise linear map from an input space to an output space. The weights in the neural network determine a partitioning of the input space into convex polytopes, where each polytope is associated with a distinct affine mapping. The structure of this partitioning, together with the affine map attached to each polytope, can be analyzed to investigate the behavior of the associated neural network. We investigate simple problems to build intuition on how these regions act and both how they can potentially be reduced in number and how similar structures occur across different networks. To validate these intuitions, we apply them to networks trained on MNIST to demonstrate similarity between those networks and the potential for them to be reduced in complexity.},
  langid = {english},
  keywords = {Jacobian matrices,linear mapping,Linearization,neural networks,Polyhedral decomposition,RELU},
  file = {/home/johan/Zotero/storage/7MTD5DY8/Sattelberg et al. - 2023 - Locally linear attributes of ReLU neural networks.pdf}
}

@misc{serraBoundingCountingLinear2018,
  title = {Bounding and {{Counting Linear Regions}} of {{Deep Neural Networks}}},
  author = {Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
  year = {2018},
  month = sep,
  number = {arXiv:1711.02114},
  eprint = {1711.02114},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.02114},
  urldate = {2025-01-20},
  abstract = {We investigate the complexity of deep neural networks (DNN) that represent piecewise linear (PWL) functions. In particular, we study the number of linear regions, i.e. pieces, that a PWL function represented by a DNN can attain, both theoretically and empirically. We present (i) tighter upper and lower bounds for the maximum number of linear regions on rectifier networks, which are exact for inputs of dimension one; (ii) a first upper bound for multi-layer maxout networks; and (iii) a first method to perform exact enumeration or counting of the number of regions by modeling the DNN with a mixed-integer linear formulation. These bounds come from leveraging the dimension of the space defining each linear region. The results also indicate that a deep rectifier network can only have more linear regions than every shallow counterpart with same number of neurons if that number exceeds the dimension of the input.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/5YTN5YU3/Serra et al. - 2018 - Bounding and Counting Linear Regions of Deep Neura.pdf}
}

@misc{shwartz-zivOpeningBlackBox2017,
  title = {Opening the {{Black Box}} of {{Deep Neural Networks}} via {{Information}}},
  author = {{Shwartz-Ziv}, Ravid and Tishby, Naftali},
  year = {2017},
  month = apr,
  number = {arXiv:1703.00810},
  eprint = {1703.00810},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.00810},
  urldate = {2025-01-20},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work [Tishby and Zaslavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/johan/Zotero/storage/EPNBT9D7/Shwartz-Ziv and Tishby - 2017 - Opening the Black Box of Deep Neural Networks via .pdf}
}

@misc{tishbyDeepLearningInformation2015a,
  title = {Deep {{Learning}} and the {{Information Bottleneck Principle}}},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  year = {2015},
  month = mar,
  number = {arXiv:1503.02406},
  eprint = {1503.02406},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.02406},
  urldate = {2025-01-20},
  abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/johan/Zotero/storage/6YVR5ZXU/Tishby and Zaslavsky - 2015 - Deep Learning and the Information Bottleneck Princ.pdf}
}
