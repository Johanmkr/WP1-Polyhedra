{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.optimize import linprog\n",
    "from typing import Dict, List, Optional, Tuple, Iterable\n",
    "from scipy.optimize import linprog\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "sys.path.append('../')\n",
    "from src_experiment import get_moons_data, train_model, NeuralNet\n",
    "from geobin import Region, Tree, RegionTree, TreeNode\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = {}\n",
    "tot_start = time.time()\n",
    "for epoch in [0,10,20,30,40]:\n",
    "    state_dict_path = get_test_data() / \"state_dicts\" / f\"epoch{epoch}.pth\"\n",
    "    state = torch.load(state_dict_path)\n",
    "    start = time.time()\n",
    "    print(f\"\\n--- Epoch {epoch} ---\")\n",
    "    tree = Tree(state)\n",
    "    tree.construct_tree(verbose=True)\n",
    "    trees[epoch] = tree\n",
    "    end = time.time()\n",
    "    print(f\"Duration: {end-start:.2f} s\")\n",
    "tot_end = time.time()\n",
    "print(f\"Total duration: {tot_end-tot_start:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7cb37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Deep Crawler (Max 10000 regions) ---\n",
      "Found 50 regions... (Just crossed Layer 1)\n",
      "Found 100 regions... (Just crossed Layer 0)\n",
      "Found 150 regions... (Just crossed Layer 1)\n",
      "Found 200 regions... (Just crossed Layer 0)\n",
      "Found 250 regions... (Just crossed Layer 0)\n",
      "Found 300 regions... (Just crossed Layer 0)\n",
      "Found 350 regions... (Just crossed Layer 1)\n",
      "Found 400 regions... (Just crossed Layer 0)\n",
      "Found 450 regions... (Just crossed Layer 0)\n",
      "Found 500 regions... (Just crossed Layer 0)\n",
      "Found 550 regions... (Just crossed Layer 1)\n",
      "Found 600 regions... (Just crossed Layer 2)\n",
      "Found 650 regions... (Just crossed Layer 0)\n",
      "Found 700 regions... (Just crossed Layer 0)\n",
      "Found 750 regions... (Just crossed Layer 0)\n",
      "Found 800 regions... (Just crossed Layer 2)\n",
      "Found 850 regions... (Just crossed Layer 0)\n",
      "Found 900 regions... (Just crossed Layer 0)\n",
      "Found 950 regions... (Just crossed Layer 1)\n",
      "Found 1000 regions... (Just crossed Layer 0)\n",
      "Found 1050 regions... (Just crossed Layer 0)\n",
      "Found 1100 regions... (Just crossed Layer 1)\n",
      "Found 1150 regions... (Just crossed Layer 1)\n",
      "Found 1200 regions... (Just crossed Layer 1)\n",
      "Found 1250 regions... (Just crossed Layer 2)\n",
      "Found 1300 regions... (Just crossed Layer 0)\n",
      "Found 1350 regions... (Just crossed Layer 2)\n",
      "Found 1400 regions... (Just crossed Layer 1)\n",
      "Found 1450 regions... (Just crossed Layer 2)\n",
      "Found 1500 regions... (Just crossed Layer 0)\n",
      "Found 1550 regions... (Just crossed Layer 0)\n",
      "Found 1600 regions... (Just crossed Layer 1)\n",
      "Found 1650 regions... (Just crossed Layer 0)\n",
      "Found 1700 regions... (Just crossed Layer 0)\n",
      "Found 1750 regions... (Just crossed Layer 1)\n",
      "Found 1800 regions... (Just crossed Layer 2)\n",
      "Found 1850 regions... (Just crossed Layer 2)\n",
      "Found 1900 regions... (Just crossed Layer 0)\n",
      "Found 1950 regions... (Just crossed Layer 0)\n",
      "Found 2000 regions... (Just crossed Layer 2)\n",
      "Found 2050 regions... (Just crossed Layer 0)\n",
      "Found 2100 regions... (Just crossed Layer 0)\n",
      "Found 2150 regions... (Just crossed Layer 2)\n",
      "Found 2200 regions... (Just crossed Layer 0)\n",
      "Found 2250 regions... (Just crossed Layer 1)\n",
      "Found 2300 regions... (Just crossed Layer 0)\n",
      "Found 2350 regions... (Just crossed Layer 2)\n",
      "Found 2400 regions... (Just crossed Layer 1)\n",
      "Found 2450 regions... (Just crossed Layer 1)\n",
      "Found 2500 regions... (Just crossed Layer 0)\n",
      "Found 2550 regions... (Just crossed Layer 0)\n",
      "Found 2600 regions... (Just crossed Layer 2)\n",
      "Found 2650 regions... (Just crossed Layer 1)\n",
      "Found 2700 regions... (Just crossed Layer 2)\n",
      "Found 2750 regions... (Just crossed Layer 1)\n",
      "Found 2800 regions... (Just crossed Layer 0)\n",
      "Found 2850 regions... (Just crossed Layer 0)\n",
      "Found 2900 regions... (Just crossed Layer 0)\n",
      "Found 2950 regions... (Just crossed Layer 0)\n",
      "Found 3000 regions... (Just crossed Layer 2)\n",
      "Found 3050 regions... (Just crossed Layer 0)\n",
      "Found 3100 regions... (Just crossed Layer 0)\n",
      "Found 3150 regions... (Just crossed Layer 0)\n",
      "Found 3200 regions... (Just crossed Layer 0)\n",
      "Found 3250 regions... (Just crossed Layer 0)\n",
      "Found 3300 regions... (Just crossed Layer 2)\n",
      "Found 3350 regions... (Just crossed Layer 0)\n",
      "Found 3400 regions... (Just crossed Layer 1)\n",
      "Found 3450 regions... (Just crossed Layer 0)\n",
      "Found 3500 regions... (Just crossed Layer 2)\n",
      "Found 3550 regions... (Just crossed Layer 0)\n",
      "Found 3600 regions... (Just crossed Layer 0)\n",
      "Found 3650 regions... (Just crossed Layer 0)\n",
      "Found 3700 regions... (Just crossed Layer 0)\n",
      "Found 3750 regions... (Just crossed Layer 0)\n",
      "Found 3800 regions... (Just crossed Layer 1)\n",
      "Found 3850 regions... (Just crossed Layer 0)\n",
      "Found 3900 regions... (Just crossed Layer 0)\n",
      "Found 3950 regions... (Just crossed Layer 0)\n",
      "\n",
      "Final Count: 3986 unique activation patterns found.\n",
      "Sample Code Structure (Tuple of Tuples): \n",
      " L1: (np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0)) \n",
      " L2: (np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)) \n",
      " L3: (np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class DeepPartitionExplorer:\n",
    "    def __init__(self, input_dim, layer_sizes, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.input_dim = input_dim\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # Initialize random weights\n",
    "        prev_dim = input_dim\n",
    "        for size in layer_sizes:\n",
    "            # He initialization for stability\n",
    "            w = np.random.randn(size, prev_dim) * np.sqrt(2/prev_dim)\n",
    "            b = np.random.randn(size) * 0.1\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "            prev_dim = size\n",
    "\n",
    "    def get_activation_pattern(self, x):\n",
    "        \"\"\"Standard forward pass to get the binary code.\"\"\"\n",
    "        code = []\n",
    "        h = x\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            pre = h @ W.T + b\n",
    "            mask = (pre > 0).astype(int)\n",
    "            code.append(tuple(mask))\n",
    "            h = pre * mask # ReLU\n",
    "        return tuple(code)\n",
    "\n",
    "    def get_all_effective_boundaries(self, x):\n",
    "        \"\"\"\n",
    "        THE TRICK: Computes the 'Effective' W and b for every neuron \n",
    "        in every layer, relative to the input space at point x.\n",
    "        \n",
    "        Returns:\n",
    "            boundaries: list of dicts {'normal': vector, 'bias': scalar, 'layer': int}\n",
    "            distances: list of floats (signed distance to each boundary)\n",
    "        \"\"\"\n",
    "        boundaries = []\n",
    "        distances = []\n",
    "        \n",
    "        # Current effective transformation: Starts as Identity (x -> x)\n",
    "        # We model the input as: W_curr * x + b_curr\n",
    "        W_curr = np.eye(self.input_dim)\n",
    "        b_curr = np.zeros(self.input_dim)\n",
    "        \n",
    "        # Current actual activation (to calculate masks)\n",
    "        h_curr = x\n",
    "        \n",
    "        for layer_idx, (W, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            # 1. Calculate the 'Effective' parameters for this layer's pre-activations\n",
    "            # z = W * (W_curr * x + b_curr) + b\n",
    "            # z = (W @ W_curr) * x + (W @ b_curr + b)\n",
    "            \n",
    "            W_effective = W @ W_curr\n",
    "            b_effective = W @ b_curr + b\n",
    "            \n",
    "            # 2. Collect boundaries for this layer\n",
    "            # Each neuron j defines a plane: W_eff[j] . x + b_eff[j] = 0\n",
    "            # We calculate distance to this plane for point x\n",
    "            \n",
    "            # Norm of each effective weight row (needed for true distance)\n",
    "            norms = np.linalg.norm(W_effective, axis=1)\n",
    "            \n",
    "            # Avoid division by zero for dead neurons (norm ~ 0)\n",
    "            valid_mask = norms > 1e-6\n",
    "            \n",
    "            # Signed distance: (w.x + b) / ||w||\n",
    "            # Note: W_effective @ x + b_effective is exactly the pre-activation value\n",
    "            dists = (W_effective @ x + b_effective) / norms\n",
    "            \n",
    "            for i in range(len(dists)):\n",
    "                if valid_mask[i]:\n",
    "                    boundaries.append({\n",
    "                        'normal': W_effective[i] / norms[i], # Unit normal\n",
    "                        'dist_val': dists[i],\n",
    "                        'layer': layer_idx,\n",
    "                        'neuron': i\n",
    "                    })\n",
    "                    distances.append(dists[i])\n",
    "            \n",
    "            # 3. Update state for the NEXT layer\n",
    "            # We must apply the ReLU mask of the current point x\n",
    "            # h_next = ReLU(z)\n",
    "            \n",
    "            # Calculate actual pre-activation to find the mask\n",
    "            z_actual = W @ h_curr + b # Note: using actual local h_curr, not global x\n",
    "            mask = (z_actual > 0).astype(float)\n",
    "            \n",
    "            # Update h_curr for the next loop iteration (standard forward pass step)\n",
    "            h_curr = z_actual * mask\n",
    "            \n",
    "            # Update the Global Linearization (Chain Rule equivalent)\n",
    "            # The next layer sees: W_next * (Mask * (W_curr * x + b_curr)) + b_next\n",
    "            # So we multiply rows of W_curr and b_curr by the mask\n",
    "            \n",
    "            # Diagonal mask matrix multiplication is equivalent to row-wise scaling\n",
    "            W_curr = mask[:, None] * (W @ W_curr) # Broadcast mask across columns\n",
    "            b_curr = mask * (W @ b_curr + b)      # Element-wise multiply\n",
    "            \n",
    "        return boundaries, np.array(distances)\n",
    "\n",
    "    def crawl_deep(self, start_point=None, max_regions=1000, steps_per_point=5):\n",
    "        if start_point is None:\n",
    "            start_point = np.zeros(self.input_dim)\n",
    "\n",
    "        queue = collections.deque([start_point])\n",
    "        discovered_codes = set()\n",
    "        discovered_codes.add(self.get_activation_pattern(start_point))\n",
    "        \n",
    "        # Keep track of unique centers to avoid looping\n",
    "        visited_hashes = set()\n",
    "        \n",
    "        print(f\"--- Deep Crawler (Max {max_regions} regions) ---\")\n",
    "        \n",
    "        while queue and len(discovered_codes) < max_regions:\n",
    "            curr_x = queue.popleft()\n",
    "            \n",
    "            # 1. Get ALL boundaries (Layer 1, 2, 3...)\n",
    "            bounds, dists = self.get_all_effective_boundaries(curr_x)\n",
    "            \n",
    "            # 2. Sort by absolute distance (find closest walls)\n",
    "            sorted_indices = np.argsort(np.abs(dists))\n",
    "            \n",
    "            # 3. Try to cross the 'k' closest walls\n",
    "            count = 0\n",
    "            for idx in sorted_indices:\n",
    "                if count >= steps_per_point: break\n",
    "                \n",
    "                b_info = bounds[idx]\n",
    "                dist = b_info['dist_val']\n",
    "                normal = b_info['normal']\n",
    "                \n",
    "                # Step: Move slightly past the wall\n",
    "                # If dist is positive, wall is behind us (relative to normal), go negative direction?\n",
    "                # Actually dist is signed. If dist is +5, we need to move -5.01 * normal.\n",
    "                # If dist is -5, we need to move +5.01 * normal.\n",
    "                # Formula: x_new = x - (dist + sign(dist)*epsilon) * normal\n",
    "                \n",
    "                epsilon = 1e-4 # Tiny step\n",
    "                step_vector = - (dist + np.sign(dist)*epsilon) * normal\n",
    "                new_x = curr_x + step_vector\n",
    "                \n",
    "                # Verify\n",
    "                new_code = self.get_activation_pattern(new_x)\n",
    "                \n",
    "                # Hash the code to store it\n",
    "                if new_code not in discovered_codes:\n",
    "                    discovered_codes.add(new_code)\n",
    "                    queue.append(new_x)\n",
    "                    count += 1\n",
    "                    \n",
    "                    # Optional: Print progress\n",
    "                    if len(discovered_codes) % 50 == 0:\n",
    "                        print(f\"Found {len(discovered_codes)} regions... (Just crossed Layer {b_info['layer']})\")\n",
    "                        \n",
    "        return discovered_codes\n",
    "\n",
    "# ==========================================\n",
    "# Run the Deep Crawler\n",
    "# ==========================================\n",
    "\n",
    "# 1. Setup: 2 Inputs -> 10 Hidden -> 10 Hidden -> 5 Hidden\n",
    "net = DeepPartitionExplorer(input_dim=2, layer_sizes=[50, 20, 20])\n",
    "\n",
    "# 2. Crawl\n",
    "regions = net.crawl_deep(max_regions=10000)\n",
    "\n",
    "print(f\"\\nFinal Count: {len(regions)} unique activation patterns found.\")\n",
    "\n",
    "# 3. Verify Complexity\n",
    "# Let's inspect a few random codes to see if deep layers are actually changing\n",
    "sample = list(regions)[0]\n",
    "print(f\"Sample Code Structure (Tuple of Tuples): \\n L1: {sample[0]} \\n L2: {sample[1]} \\n L3: {sample[2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polyhedra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
