{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab6cdc5",
   "metadata": {},
   "source": [
    "# Basic Ideas of Finding Regions\n",
    "Detailed exploration of the method proposed in [this paper](https://proceedings.mlr.press/v221/liu23a.html) by Liu et al. but focused on a single-layer approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024660f3",
   "metadata": {},
   "source": [
    "## Simulating a ReLU layer\n",
    "\n",
    "We start by simulating a ReLU layer from $\\mathbb{R}^2\\to\\mathbb{R}^3$. This require a $(3\\times2)$ weight matrix $W$ and a $(3\\times1)$ bias vector $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860b9f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[ 0.49671415 -0.1382643 ]\n",
      " [ 0.64768854  1.52302986]\n",
      " [-0.23415337 -0.23413696]]\n",
      "b: [[ 1.57921282]\n",
      " [ 0.76743473]\n",
      " [-0.46947439]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "W = np.random.randn(3,2)\n",
    "b = np.random.randn(3,1)\n",
    "print(\"W:\", W)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb42de",
   "metadata": {},
   "source": [
    "We also need a random point in $\\mathbb{R}^2$ and a ReLU function, for now we implement the ReLU function and its derivative brute force as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{ReLU}(a) = \\begin{cases}\n",
    "        a \\; \\text{if}\\; a>0 \\\\\n",
    "        0 \\; \\text{if}\\; a\\leq 0.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{dReLU}(a) = \\begin{cases}\n",
    "        1 \\; \\text{if}\\; a>0 \\\\\n",
    "        0 \\; \\text{if}\\; a\\leq 0.\n",
    "    \\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2ab7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(a):\n",
    "    return np.maximum(a, 0)\n",
    "\n",
    "def dReLU(a):\n",
    "    return np.where(a > 0, 1, 0)\n",
    "\n",
    "# Random point in R^2\n",
    "x = np.random.randn(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceac325",
   "metadata": {},
   "source": [
    "Next, we compute the activation $a=Wx+b$ of an input $x$, where $a\\in\\mathbb{R}^3$, the layer output $z=\\text{ReLU}(a)$ and the derivative $q=\\text{dReLU}(a)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85dc338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation a:\n",
      " [[ 1.91278419]\n",
      " [ 0.41304567]\n",
      " [-0.48801344]]\n",
      "Output z:\n",
      " [[1.91278419]\n",
      " [0.41304567]\n",
      " [0.        ]]\n",
      "Gradient q:\n",
      " [[1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "activation = W @ x + b\n",
    "print(\"Activation a:\\n\", activation)\n",
    "z = ReLU(activation)\n",
    "print(\"Output z:\\n\", z)\n",
    "q = dReLU(activation)\n",
    "print(\"Gradient q:\\n\", q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce3b75",
   "metadata": {},
   "source": [
    "## Linear Model for gradients\n",
    "\n",
    "The gradients of ReLU layers are binary vector with entries of $0$ and $1$. This can be turned into a sign vector with entries $-1$ and $1$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{SIGN}(a) = -2q(a) + 1\n",
    "\\end{equation*}\n",
    "\n",
    "This will map a vector $(1,0)\\to(-1,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f93ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign vector:\n",
      " [[-1]\n",
      " [-1]\n",
      " [ 1]]\n"
     ]
    }
   ],
   "source": [
    "def SIGN_from_activation(a):\n",
    "    return -2 * dReLU(a) + 1\n",
    "def SIGN_from_gradient(grad):\n",
    "    return np.where(grad > 0, -1, 1)\n",
    "\n",
    "assert all(SIGN_from_activation(activation) == SIGN_from_gradient(dReLU(activation)))\n",
    "sign_vector = SIGN_from_activation(activation)\n",
    "print(\"Sign vector:\\n\", sign_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ddb0c",
   "metadata": {},
   "source": [
    "This is useful because any datapoint $x$ with gradient $q$ and corresponding sign vector $s'$ satisfies the following linearity:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{diag}(s')(Wx+b) \\leq 0\n",
    "\\end{equation*}\n",
    "\n",
    "Expanding this to:\n",
    "\n",
    "\\begin{align*}\n",
    "    A &= \\text{diag}(s')W \\\\\n",
    "    c &= -\\text{diag}(s')b\n",
    "\\end{align*}\n",
    "\n",
    "enables us to write this in a more compact way as\n",
    "\n",
    "\\begin{equation*}\n",
    "    Ax\\leq c\n",
    "\\end{equation*}.\n",
    "\n",
    "So, each gradient vector $q$ gives rise to a convex regions, described by the solution of the linear program above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amat:\n",
      " [[-0.49671415  0.1382643 ]\n",
      " [-0.64768854 -1.52302986]\n",
      " [-0.23415337 -0.23413696]]\n",
      "Cmat:\n",
      " [[1.57921282]\n",
      " [0.76743473]\n",
      " [0.46947439]]\n"
     ]
    }
   ],
   "source": [
    "Amat = np.diag(sign_vector.flatten()) @ W \n",
    "cmat = -np.diag(sign_vector.flatten()) @ b\n",
    "\n",
    "print(\"Amat:\\n\", Amat)\n",
    "print(\"Cmat:\\n\", cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754de656",
   "metadata": {},
   "source": [
    "## Feasibility\n",
    "\n",
    "The next step is to determine of the region is feasible at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb78024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def check_feasibility_torch(A, c, max_iters=200, tol=1e-4, input_bound=None):\n",
    "    if not isinstance(A, torch.Tensor):\n",
    "        A = torch.tensor(A, dtype=torch.float32)\n",
    "    if not isinstance(c, torch.Tensor):\n",
    "        c = torch.tensor(c, dtype=torch.float32)\n",
    "\n",
    "    m = A.shape[1]  # input dimension\n",
    "    x = torch.randn((m, 1), requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([x], lr=0.05)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Original inequality\n",
    "        constraint_violations = A @ x - c\n",
    "        \n",
    "        if input_bound is not None:\n",
    "            # Bounding box constraints: -bound <= x <= bound\n",
    "            lower_bounds = torch.relu(-x - input_bound)\n",
    "            upper_bounds = torch.relu(x - input_bound)\n",
    "            \n",
    "            loss = constraint_violations.sum() + lower_bounds.sum() + upper_bounds.sum()\n",
    "        else:\n",
    "            loss = torch.relu(constraint_violations).sum()\n",
    "            \n",
    "        if loss.item() < tol:\n",
    "            return True\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68c0a084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_feasibility_torch(Amat,cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "834272de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat but for arbitrary bit vector:\n",
    "\n",
    "def region_for_bit_vector(bit_vector):\n",
    "    sign_vector = SIGN_from_gradient(np.array(bit_vector))\n",
    "    Amat = np.diag(sign_vector.flatten()) @ W \n",
    "    cmat = -np.diag(sign_vector.flatten()) @ b\n",
    "\n",
    "    return Amat, cmat if check_feasibility_torch(Amat, cmat) else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "946f9975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feasibility for bit-vector (0, 0, 0): False\n",
      "Feasibility for bit-vector (0, 0, 1): True\n",
      "Feasibility for bit-vector (0, 1, 0): True\n",
      "Feasibility for bit-vector (0, 1, 1): True\n",
      "Feasibility for bit-vector (1, 0, 0): True\n",
      "Feasibility for bit-vector (1, 0, 1): True\n",
      "Feasibility for bit-vector (1, 1, 0): True\n",
      "Feasibility for bit-vector (1, 1, 1): True\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def generate_all_bit_vectors(bit_length):\n",
    "    return list(itertools.product([0, 1], repeat=bit_length))\n",
    "\n",
    "for bit_vector in generate_all_bit_vectors(3):\n",
    "    region_for_bit_vector(bit_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f3d5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f10b4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def vertices_from_H_enumeration(A, c, tol=1e-9):\n",
    "    \"\"\"\n",
    "    Compute vertices of P = { x | A x <= c } in R^n by enumerating combinations of n constraints.\n",
    "    A: (k, n) numpy array\n",
    "    c: (k, 1) or (k,) numpy array\n",
    "    Returns: (m, n) array of unique vertices (m may be zero)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    c = np.asarray(c, dtype=float).reshape(-1)\n",
    "    k, n = A.shape\n",
    "    vertices = []\n",
    "\n",
    "    # enumerate combinations of n rows\n",
    "    for idxs in itertools.combinations(range(k), n):\n",
    "        A_sub = A[list(idxs), :]\n",
    "        # skip singular subsets\n",
    "        if np.linalg.matrix_rank(A_sub) < n:\n",
    "            continue\n",
    "        c_sub = c[list(idxs)]\n",
    "        try:\n",
    "            x = np.linalg.solve(A_sub, c_sub)\n",
    "        except np.linalg.LinAlgError:\n",
    "            continue\n",
    "        # feasibility test (allow small tolerance)\n",
    "        if np.all(A.dot(x) <= c + 1e-8 + tol):\n",
    "            vertices.append(np.round(x, 12))  # round to reduce numerical dupes\n",
    "\n",
    "    if not vertices:\n",
    "        return np.zeros((0, n))\n",
    "\n",
    "    verts = np.unique(np.vstack(vertices), axis=0)\n",
    "    return verts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cf979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.92359839  0.91867617]\n",
      " [-2.61172976  0.60678568]]\n"
     ]
    }
   ],
   "source": [
    "print(vertices_from_H_enumeration(Amat,cmat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251fc72",
   "metadata": {},
   "source": [
    "## Multiple Layers\n",
    "\n",
    "### Recursive weight matrices\n",
    "\n",
    "They propose a recursive weight matrices and bias vectors for layer $l$: \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\hat{W}_l = W_l\\text{diag}(q_{l-1})\\hat{W}_{l-1}\n",
    "\\end{equation*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{b}_l = W_l\\text{diag}(q_{l-1})\\hat{b}_{l-1}\n",
    "\\end{equation}\n",
    "for $2\\leq l \\leq L$ and $\\hat{W}_1=W_1$ and $\\hat{b}_1 = b_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c70d0",
   "metadata": {},
   "source": [
    "## Convex regions (polytopes/polyhedra)\n",
    "\n",
    "The central idea is that points that share the same gradient $q$ belong to the same convex regions. We therefore define a convex polytope $\\mathcal{P}$ in $\\mathbb{R}^2$ (input space) as the set $\\{ x'| q(x') = q(x) \\forall x'\\in\\mathbb{R}^2 \\}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polyhedra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
